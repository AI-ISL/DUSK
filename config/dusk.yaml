# model Config
model_family: llama3-8b
model_path: meta-llama/Meta-Llama-3-8B

use_LoRA: false
LoRA:
  r: 8
  alpha: 32
  dropout: 0.05

# dataset config
forget_data: Prof
data_path: data/Prof
task_id: 1

# unlearning config
forget_loss: NONE+GD
lr: 1e-5
num_epochs: 5
batch_size: 4
gradient_accumulation_steps: 4
forget_coeff: 0.1 # 0.1 for ME+GD, 1.0 for baselines
regularization_coeff: 1.0

forget_type: formats
num_formats: 5

beta: 0.1
weight_decay: 0.01

fix_ref_model: false

seed: 1001

# save config
save_checkpoint: false
overwrite_dir: false
save_steps: last # steps_per_epoch
save_root: results

save_dir: ${save_root}/${model_family}/${forget_loss}/seed_${seed}/epoch${num_epochs}_${lr}_task${forget_type}_${num_formats}

# eval config
ds_size: 300
eval_unlearn_step: last
